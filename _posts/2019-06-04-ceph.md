---
layout:      post
title:       "Ceph 增加和删除 osd"
categories:  [运维, 大数据, Ceph]
description: "Ceph 增加和删除 osd"
keywords:    运维, 大数据, Ceph
---

# Ceph 增加和删除 osd

## 增加 osd

### 在新节点安装 ceph

在新节点上安装 ceph 软件包，保证和 ceph 集群的软件版本一致

```sh
ceph-deploy install [host-name] --release luminous
```

上面是通用做法，在离线环境下，需要准备好rpm包，然后修改本地ceph的yum，然后在每个节点分别执行

```sh
yum instal -y ceph-deploy
yum instal -y ceph
```

### 列出新节点上所有可用磁盘

```sh
ceph-deploy disk list [host-name]
```

### 发送配置文件

将配置文件和管理秘钥发送到新的节点

(此操作要在安装目录中执行，例如：/usr/local/ceph-cluster)

```sh
ceph-deploy admin [host-name]
```

### 添加 osd

__注意：__ 此方式适合于使用裸盘进行添加，如果要在LVM卷上创建 osd，则参数 `--data` 必须是 `volume_group/lv_name`，而不是卷的块设备的路径。

(此操作要在安装目录中执行，例如：/usr/local/ceph-cluster)

```sh
ceph-deploy osd create --data /dev/sdb [host-name]
```

### 观察平衡过程

当新的节点加入集群，ceph 集群开始将部分现有的数据重新平衡到新加入的 osd 上，用下面的命令可用观察平衡过程。

```sh
ceph -w
watch ceph -s
watch ceph health
```

### 检查集群的存储容量

```sh
rados df
```

### 查看新加入的 osd

```sh
ceph osd tree
```

__注意：__ 在生产环境中，一般不会再新节点加入 ceph 集群后，立即开始数据回填，这样会影响集群性能。所以我们需要设置一些标志位，来完成这个目的。

```sh
ceph osd set noin
ceph osd set nobackfill
```

在用户访问的非高峰时，取消这些标志位，集群开始在平衡任务。

```sh
ceph osd unset noin
ceph osd unset nobackfill
```

## 删除 osd

### 进入要删除 osd 的主机

### 将 osd 的权重标记为 `0`

```sh
ceph osd crush reweight osd.{osd-num} 0
```

__注:__ `osd-num` 通过 `ceph osd tree` 查看

### 观察数据迁移

```sh
ceph -w
watch ceph -s
```

__注:__ 待集群状态正常后再进行下一步操作

### 踢出集群

```sh
ceph osd out {osd-num}
```

__注:__ 观察集群状态，待集群状态正常后再进行下一步操作

### 通过 `systemctl` 停止 osd 服务

```sh
systemctl status ceph-osd@{osd-num}
systemctl stop ceph-osd@{osd-num}
systemctl disable ceph-osd@{osd-num}
systemctl status ceph-osd@{osd-num}
```

### 删除 CRUSH 图的对应 OSD 条目

```sh
ceph osd crush remove osd.{osd-num}
```

__注:__ 删除 CRUSH 图的对应 OSD 条目，它就不再接收数据了。也可以反编译 CRUSH 图、删除 device 列表条目、删除对应的 host 桶条目或删除 host 桶（如果它在 CRUSH 图里，而且你想删除主机），重编译 CRUSH 图并应用它。

### 删除 OSD 认证密钥

```sh
ceph auth del osd.{osd-num}
```

### 删除 OSD

```sh
ceph osd rm {osd-num}
```

### 处理配置文件

如果 `ceph.conf` 中记载了相关的内容，则需要修改配置文件并更新到其他主机，如果没有，则忽略此步骤。

### 卸载 osd 挂载的磁盘

```sh
df -h
umount /var/lib/ceph/osd/ceph-{osd-num}
ceph-disk zap /dev/s{?}{?}
```

### 清除主机

如果只想清除 /var/lib/ceph 下的数据、并保留 Ceph 安装包，可以用 purgedata 命令。

```sh
ceph-deploy purgedata {hostname} [{hostname} ...]
```

要清理掉 /var/lib/ceph 下的所有数据、并卸载 Ceph 软件包，用 purge 命令。

```sh
ceph-deploy purge {hostname} [{hostname} ...]
```

### 修改 crushmap

剔除完 osd 之后，使用 `ceph -s` 状态正常，但是使用 `ceph osd tree` 查看列表的时，却发现已经没有osd的主机依然存在。

原因是 `crushmap` 的一些架构图，不会动态修改，所以需要手工的修改 `crushmap`。

```sh
ceph osd  getcrushmap -o old.map  //导出crushmap
crushtool -d old.map -o old.txt   //将crushmap转成文本文件，方便vim修改
cp old.txt new.txt        //做好备份
```

修改 new.txt, 将此 host 相关的内容删除

```sh
  host host-name {
xxxxx
xxxxx
}
  host host-name {
xxxxx
xxxxx
}
root default {
item host-name weight 0.000
item host-name weight 0.000
}
```

把以上的内容删除掉后，重新编译为二进制文件，并将二进制 map 应用到当前 map 中

```sh
curshtool -c new.txt -o new.map
ceph osd setcrushmap -i new.map
```

此时ceph会重新收敛，等待收敛完毕后，ceph可正常使用
